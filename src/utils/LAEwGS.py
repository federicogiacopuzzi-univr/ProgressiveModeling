import itertools
import numpy as np
import tensorflow as tf
import pandas as pd
from tensorflow.keras import layers, models
from tensorflow.keras import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

def create_autoencoder_model(input_dim, latent_dim, neurons_per_layer):
    r"""
    Creates and compiles an autoencoder model for dimensionality reduction and data reconstruction.

    Parameters
    ----------
    - input_dim (int): The dimension of the input (number of features).
    - latent_dim (int): The dimension of the latent code (reduced dimension).
    - neurons_per_layer (int): The number of neurons for each hidden layer (default=64).

    Returns
    ----------
    - model (tensorflow.keras.Sequential): A compiled model ready for training.

    The function performs the following steps:
    1. Creates an autoencoder model with an encoder and a decoder.
    2. Uses a two-layer architecture with the specified latent dimension.
    3. Compiles the model with the Adam optimizer and MSE loss function.
    """
    # Create a Sequential model for the autoencoder
    model = models.Sequential()
    
    # Encoder
    model.add(layers.Dense(neurons_per_layer, input_dim=input_dim, activation='relu'))  # Encoder layer 1
    model.add(layers.Dense(latent_dim, activation='relu'))  # Bottleneck (latent space)
    
    # Decoder
    model.add(layers.Dense(neurons_per_layer, activation='relu'))  # Encoder layer 1
    model.add(layers.Dense(input_dim, activation='sigmoid'))  # Output layer (reconstruction)
    
    # Compile the model with Adam optimizer and MSE loss function
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[metrics.mae])

    return model

def Autoencoder(features,
                dataframe = None, 
                train_df = None, 
                test_df = None,
                epochs = 100,
                latent_dim = 20,
                test_size = 0.3,
                random_state = 7,
                validation_split = None,
                neurons_per_layer = 64,
                batch_size = 32
               ):
    r"""
    Creates and trains an autoencoder model to reduce the dimensionality of the data.

    Parameters
    ----------
    - features (list): List of column names representing the features used as input to the autoencoder.
    - dataframe (pandas.DataFrame): A DataFrame containing the data, including both features (optional).
    - train_df (pandas.DataFrame or None): The training dataset (optional).
    - test_df (pandas.DataFrame or None): The test dataset (optional).
    - latent_dim (int): The dimension of the latent code (default=20).
    - epochs (int): The number of epochs to train the model (default=100).
    - test_size (float): The proportion of the test set (default=0.3).
    - random_state (int): The seed for the dataset splitting (default=7).
    - validation_split (float or None): The proportion of the data to use for validation (default=None, which disables
      validation).
    - neurons_per_layer (int): The number of neurons for each hidden layer (default=64).
    - batch_size (int): The number of samples per gradient update during training (default=32).

    Returns
    ----------
    - tuple: A tuple containing:
        - X_test (numpy.ndarray): The original test data.
        - x_pred (numpy.ndarray): The autoencoder's predictions on the test data.

    The function performs the following steps:
    1. Standardizes the features to the range [0, 1] using MinMaxScaler.
    2. Splits the data into training and test sets if no explicit train_df and test_df are provided. If these datasets are
       given, it uses them directly for training and testing.
    3. Creates and trains the autoencoder model.
    4. Returns the original test data and the predictions generated by the model.
    """
    # If df is provided, use it to create train and test splits
    if dataframe is not None:
        # Standardize the features
        sc = MinMaxScaler(feature_range=(0, 1))
        dataframe[features] = sc.fit_transform(dataframe[features])

        # Extract the features
        X = dataframe[features].to_numpy()

        # Split the data into training and test sets
        X_train, X_test = train_test_split(X, test_size=test_size, random_state=random_state)
    elif train_df is not None and test_df is not None:
        # Preprocessing for train_df and test_df
        sc = MinMaxScaler(feature_range=(0, 1))
        train_df[features] = sc.fit_transform(train_df[features])
        test_df[features] = sc.transform(test_df[features])

        X_train = train_df[features].to_numpy()
        X_test = test_df[features].to_numpy()
    else:
        raise ValueError("Either df or both train_df and test_df must be provided.")

    # Create the autoencoder model
    model = create_autoencoder_model(input_dim=X_train.shape[1], latent_dim=latent_dim, neurons_per_layer=neurons_per_layer)

    # Train the model
    model.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, verbose=1)

    # Make predictions on the test data
    x_pred = model.predict(X_test)

    return X_test, x_pred

def grid_search_autoencoder(features, 
                            dataframe = None, 
                            train_df = None, 
                            test_df = None, 
                            latent_dims=[20], 
                            neurons_per_layer=[64], 
                            batch_sizes=[32], 
                            epochs=[100], 
                            test_size=0.3, 
                            random_state=7,
                            target_columns=None
                           ):
    """
    Perform a grid search over different configurations of the autoencoder to determine the best model.

    Parameters
    ----------
    - dataframe (pandas.DataFrame or None): A DataFrame containing the data, including both features (optional).
    - train_df (pandas.DataFrame or None): The training dataset (optional).
    - test_df (pandas.DataFrame or None): The test dataset (optional).
    - features (list): List of feature names to be used in the autoencoder.
    - latent_dims (list): List of latent dimension sizes to test.
    - neurons_per_layer (list): List of neurons per hidden layer to test.
    - batch_sizes (list): List of batch sizes to test.
    - epochs (int): Number of epochs to train each model.
    - test_size (float): Proportion of data to be used for testing.
    - random_state (int): Random seed for reproducibility.
    - target_columns (list or None): List of columns to use for evaluation. If None, use all features.
    
    Returns
    -------
    - pandas.DataFrame: A DataFrame containing the results of the grid search, including configurations and performance.
    """
    # Create all combinations of the hyperparameters
    combinations = list(itertools.product(latent_dims, neurons_per_layer, batch_sizes))

    results = []

    for latent_dim, neurons, batch_size in combinations:
        print(f"Training with latent_dim={latent_dim}, neurons={neurons}, batch_size={batch_size}")
        
        if dataframe is not None:
            X_test, x_pred = Autoencoder(
                features=features,
                dataframe=dataframe,
                latent_dim=latent_dim, 
                neurons_per_layer=neurons,
                batch_size=batch_size,
                epochs=epochs[0],
                test_size=test_size,
                random_state=random_state
            )
        elif train_df is not None and test_df is not None:
            X_test, x_pred = Autoencoder(
                features=features,
                train_df=train_df,
                test_df=test_df,
                latent_dim=latent_dim, 
                neurons_per_layer=neurons,
                batch_size=batch_size,
                epochs=epochs[0]
            )
        else:
            raise ValueError("You must provide either 'dataframe' or both 'train_df' and 'test_df'.")

        if target_columns:
            target_idx = [features.index(col) for col in target_columns]
            X_target = X_test[:, target_idx]
            x_target_pred = x_pred[:, target_idx]
        else:
            X_target = X_test
            x_target_pred = x_pred

        # Normalized errors
        mae = np.sum(np.abs(X_target - x_target_pred))
        nmae = mae / np.sum(np.abs(X_target))
        mse = np.sum(np.square(X_target - x_target_pred))
        nmse = mse / np.sum(np.square(X_target))
        nrmse = np.sqrt(nmse)
        
        results.append({
            'latent_dim': latent_dim,
            'neurons_per_layer': neurons,
            'batch_size': batch_size,
            'nmae': nmae,
            'nmse': nmse,
            'nrmse': nrmse
        })

    # Convert results to a DataFrame for easy analysis
    results_df = pd.DataFrame(results)

    # Return the best configuration based on the lowest MSE
    best_configuration = results_df.loc[results_df['nmae'].idxmin()]

    if target_columns:
        target_info = ", ".join(target_columns)
    else:
        target_info = "all features"

    print(f"\nBest Configuration (based on nmae on {target_info}):")
    print(best_configuration)

    return results_df