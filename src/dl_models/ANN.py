import numpy as np
from matplotlib import pyplot as plt
import sklearn
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, KFold
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import metrics


def create_model_large(size, X_train):
    r"""
    Creates and compiles a large artificial neural network model for regression tasks.

    Parameters
    ----------
    - size (int): The number of neurons in the input and the last hidden layer. 
                  This determines the complexity and capacity of the model.
    - X_train (numpy.ndarray): The training feature data used to determine the input dimension 
                               of the model.

    Returns
    ----------
    - model (keras.Sequential): A compiled Sequential model ready for training.

    The function performs the following operations:
    1. Initializes a Sequential model, allowing layers to be stacked sequentially.
    2. Adds an input layer with a specified number of neurons (defined by 'size') and 
       ReLU (Rectified Linear Unit) as the activation function. The input dimension is set 
       to the number of features in the training data (X_train).
    3. Adds a hidden layer with double the number of neurons compared to the input layer 
       (2 * size) and uses ReLU activation.
    4. Adds another hidden layer with the same number of neurons as the input layer (size) 
       and ReLU activation.
    5. Adds an output layer with a single neuron (suitable for regression tasks) and no 
       activation function.
    6. Compiles the model using the Adam optimizer, with mean squared error as the loss 
       function and mean absolute error (MAE) as a performance metric.

    The compiled model is then returned for training.
    """
    # Create a Sequential model, which allows layers to be stacked one after another.
    model = Sequential()
    
    # Add the input layer with 35 neurons and ReLU activation function.
    # The 'input_dim' is set to the number of features in the training data (X_train).
    model.add(Dense(size, input_dim=X_train.shape[1], activation='relu'))
    
    # Add a hidden layer with 70 neurons and ReLU activation function.
    model.add(Dense(size*2, activation='relu'))
    
    # Add another hidden layer with 35 neurons and ReLU activation function.
    model.add(Dense(size, activation='relu'))
    
    # Add the output layer with a single neuron (for regression tasks) and no activation function.
    model.add(Dense(1))
    
    # Compile the model with the Adam optimizer, mean squared error as the loss function, and mean absolute error (MAE) as a performance metric.
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[metrics.mae])
    
    # Return the compiled model.
    return model

def ANN(dataframe, features, target, n_splits = 0):
    r"""
    Constructs and trains an artificial neural network (ANN) to predict a specific target 
    based on a provided set of features. If n_splits > 1, performs K-Fold Cross Validation.

    Parameters
    ----------
    - dataframe (pandas.DataFrame): A DataFrame containing the data, including both features and the target.
    - features (list): A list of column names representing the features used as inputs to the neural network.
    - target (str): The name of the column in the DataFrame that represents the target variable to be predicted.
    - n_splits (int, optional): Number of folds for K-Fold CV. If 0 or 1, no K-Fold is applied.

    Returns
    ----------
    - tuple: A tuple containing two elements:
        - y_test (numpy.ndarray): The true values of the target variable in the test set.
        - pred (numpy.ndarray): The predictions generated by the neural network for the test set.
    
    The function performs the following operations:
    1. Standardizes the features to the range [0, 1] using MinMaxScaler.
    2. If n_splits is 0 or 1 or not specified, splits the data into training, validation, and test sets. Otherwise, applies K-Fold Cross Validation with the
        specified number of folds, splitting the data into training and test sets for each fold.
    3. Creates a neural network model using the specified architecture.
    4. Trains the model using the training data and validates it with the validation set.
    5. Generates predictions on the test set (or for each fold) and returns the true and predicted values, concatenated across folds if K-Fold is used.
    """

    df = dataframe.copy()
    sc = MinMaxScaler(feature_range=(0, 1))
    
    # Standardize features
    for var in features:
        if var != target:
            df[var] = sc.fit_transform(df[var].values.reshape(-1,1))
    
    X = df.drop(columns=target).to_numpy()
    Y = df[target].to_numpy()

    sc_Y = MinMaxScaler(feature_range=(0,1))
    Y_scaled = sc_Y.fit_transform(Y.reshape(-1, 1)).flatten()
    
    seed = 7
    np.random.seed(seed)

    if n_splits and n_splits > 1:
        # K-Fold Cross Validation
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)
        all_y_true = []
        all_y_pred = []
        
        for train_index, test_index in kf.split(X):
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = Y[train_index], Y[test_index]
            
            # Further split training into train/val
            X_train_split, X_val, y_train_split, y_val = train_test_split(
                X_train, y_train, test_size=0.2, random_state=seed
            )
            
            model = create_model_large(len(features), X_train_split)
            model.fit(X_train_split, y_train_split, epochs=150, batch_size=32,
                      validation_data=(X_val, y_val), verbose=0)
            
            pred = model.predict(X_test).reshape(-1)
            all_y_true.append(y_test)
            all_y_pred.append(pred)
        
        y_true = np.concatenate(all_y_true)
        y_pred = np.concatenate(all_y_pred)
        return y_true, y_pred
    
    else:
        # Base behavior: single train/test split
        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)
        
        model = create_model_large(len(features), X_train)
        model.fit(X_train, y_train, epochs=150, batch_size=32, validation_data=(X_val, y_val), verbose=0)
        pred = model.predict(X_test).reshape(-1)
        return y_test, pred